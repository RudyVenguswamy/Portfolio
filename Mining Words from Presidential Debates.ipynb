{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics in Presidential Debates\n",
    "\n",
    "In this notebook, I scrape data from the Presidential Debates from the Commission of Presidential Debates website: https://www.debates.org/voter-education/debate-transcripts/\n",
    "\n",
    "I analyze the counts of specific values from each of the speeches and sentiment score each of the candidate's sentiment score using Blob NLP Sentiment analysis toolkit.\n",
    "\n",
    "Using `requests` and `BeautifulSoup` to find all links/ URLs on the website and use the links found to get the text from each presidential debate.\n",
    "\n",
    "This project was inspired by a project I did for my DataX class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "source = requests.get(\"https://www.debates.org/voter-education/debate-transcripts/\") \n",
    "soup = bs.BeautifulSoup(source.content, features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllist = []\n",
    "titlelist = []\n",
    "for a in soup.find_all('a'):\n",
    "        stringurl = \"https://www.debates.org\" + a.get('href')\n",
    "        if stringurl not in urllist and 'debate-transcript' in a.get('href'):\n",
    "            urllist.append(stringurl)\n",
    "            titlelist.append(a.text)\n",
    "\n",
    "titlelist\n",
    "\n",
    "#deal with first val/last val\n",
    "#will have to combine\n",
    "#print(titlelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing first and last speeches manually (scraping isn't perfect)\n",
    "urllist = urllist[1:-1]\n",
    "titlelist = titlelist[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks whether two strings are similar to one another, to deal with typos in the labeling of the speaker's name/ inconsistencies in naming\n",
    "#This method will be used below.\n",
    "from difflib import SequenceMatcher\n",
    "def isSimilar(x,y):\n",
    "    simthreshhold = 0.5\n",
    "    if x in y:\n",
    "        return True\n",
    "    if y in x:\n",
    "        return True\n",
    "    return SequenceMatcher(None, x, y).ratio() > simthreshhold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay, so here's where the data scraping gets tricky....\n",
    "In the following block of code I go through every url of presidential debates, find each paragraph break and use heuristics like the formatting of the paragraph to figure out who is speaking. The code can seem intimidating, but that's because while scraping there were quite a few edge cases (like typos or weird formatting and inconsistencies from the website) that I had to find a system of processing robust enough to get those data points too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechlist = []\n",
    "for i in urllist:\n",
    "    #For each url in the list of presidential debates\n",
    "    source1 = requests.get(i) \n",
    "    soup1 = bs.BeautifulSoup(source1.content, features='html.parser')\n",
    "\n",
    "    speech = ''\n",
    "    speechdf = pd.DataFrame()\n",
    "\n",
    "    for p in soup1.find_all('p'):\n",
    "        currspeaker = ''\n",
    "        splittext = p.text.split(':',1)\n",
    "        \n",
    "        #if there's colon\n",
    "        if len(splittext) == 2:\n",
    "            name = splittext[0]\n",
    "            speechtext =splittext[1]\n",
    "            #if there's new speaker\n",
    "            if (name.isupper()):\n",
    "                currspeaker = name\n",
    "                #if already in list of speakers\n",
    "                if any(isSimilar(name,col) for col in speechdf.columns):\n",
    "                    for cols in speechdf.columns:\n",
    "                        if isSimilar(name, cols):\n",
    "                        \n",
    "                            currspeaker = cols\n",
    "                    #set currspeaker to similar column\n",
    "                    speechdf.loc[len(speechdf.index), currspeaker] = speechtext\n",
    "                else: #if not in list of speakers\n",
    "                    #create new column for new speaker\n",
    "                    speechdf[currspeaker] = np.NaN\n",
    "                    speechdf.loc[len(speechdf.index), currspeaker] = speechtext\n",
    "\n",
    "   \n",
    "    #continue paragraph\n",
    "        elif len(currspeaker) > 1:\n",
    "            speechdf.loc[len(speechdf.index), currspeaker] = p.text\n",
    "\n",
    "    for col in speechdf.columns:\n",
    "        if len(speechdf[col]) - speechdf[col].isna().sum() < 5: #if you only want the presidents and not reporters, set this value higher\n",
    "            speechdf = speechdf.drop(col, axis = 1)     \n",
    "    speechlist.append(speechdf)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speechlist contains a list of dataframes, with each DF representing a presidential debate\n",
    "len(speechlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for speech in speechlist:\n",
    "\n",
    "for speech in speechlist:\n",
    "    sentdf = pd.DataFrame()\n",
    "    for col in speech.columns:\n",
    "        speechstring = ''\n",
    "        for index,row in speech.iterrows():\n",
    "            if isinstance(row[col],str):\n",
    "                speechstring += row[col]\n",
    "        \n",
    "        blob = TextBlob(speechstring)\n",
    "        #angry:-1 to happy: +1\n",
    "        polarity = blob.sentiment.polarity\n",
    "        sentdf[col] = np.NaN\n",
    "        sentdf.loc[0, col] = polarity\n",
    "    display(sentdf)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how sentiment analysis works\n",
    "string = 'hello happy person'\n",
    "blob = TextBlob(string)\n",
    "print(blob.sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Data Mining of Presidential Debates\n",
    "If you look at the data frames above, you'll see a few different names like reporters sprinkled in. I commented within my cleaning method if someone wants to get rid of the reporters and other question askers from the data frame, just set the threshold for number of paragraphs spoken to a higher number. I left it in for now in case someone wanted to go through the sentiments of the questions as well. Thanks for reading through this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
